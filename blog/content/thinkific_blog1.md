Title:  How to showcase your Apache Spark skills
Subtitle:    Selecting good code projects
Project:     Learn Apache Spark
Author:      Mark Plutowski
Affiliation: Plutoware Delimited
Web:         https://pluteski.github.io
Date:        2018-07-29


# How to spotlight your Apache Spark skills 

<img id="img1" align="right" hspace=10x src="http://github.com/pluteski/speech-to-text/raw/master/blog/content/images/spark/L4.UI.DAG.png" alt="Spark UI DAG" width="230px"
/>
<br>

How do you prove your capability as a Apache Spark data scientist when you don’t have much to show? Perhaps this is because you don’t have much experience. Perhaps it is because the work you do cannot be shown. 
You may have done impressive work, but have nothing to share because most developers work for companies that don’t publish their code as open source. 

In my long and successful career 
as data scientist and machine learning developer, I have learned many useful lessons for organizing information and presenting it effectively. 
I have created online courses to help the aspiring [data scientist](https://learn-apache-spark.thinkific.com/courses/spark-sql) 
and [machine learning developer](https://learn-apache-spark.thinkific.com/courses/sparkml-features). 
This article is going to delve more into how to present yourself. 
I am going to give you some tips for bringing your skills out of the dark and into the light. 

## Publish your work
Publishing a code project is a great way to introduce yourself to an interviewer. This also provides them with ample talking points for various styles of interview questions.  Doing this makes the interviewers job easier. 
By doing so you are giving them context on what you already know, and providing talking points for interview questions. 
Interviewers in the software industry are often stretched thin; making their job easier puts them into a better mood while also demonstrating your preparedness. 

This can be helpful for many types of interview questions, including behavioral questions, situational questions, coding challenges, and system design challenges. 

## Show what you know
In this article I will show you how you can showcase your skills. By doing so, you will also learn new relevant skills along the way.  I’ll show you how publishing your work using self-explanatory visualizations can separate your from the pack.  I’ll also show how to select a coding project that demonstrates useful skills that are immediately applicable in a production setting.  Furthermore, by setting this up for yourself, you will also acquire new skills that are valuable for the coding challenge portion of the interview, as well as being directly transferable to the job itself.

## Know what you show
To utilize this approach effectively, you must be intimately familiar with every line of content in your repo. The best way to achieve this is to create an original work of your own design, developed step by step from start to finish by yourself. Once you get started this may not take as long as you expect. On the other hand, there may be steps that blossom into much more time-intensive investment than you expected.  That's actually ok -- it gives you something interesting to explain when discussing your process. 

## Hone your edge, gain a new one
Someone who has well organized and nontrivial open source code will be prioritised over other similarly experienced candidates, but because their competencies are easier to evaluate.  By providing visibility you simplify the job of the recruiters and interviewers, while simultaneously preparing yourself for the interview, as well as learning relevant skills that will be useful on the job.

## Example repo
As an example, take [this python repo](https://github.com/pluteski/first2017mp) 
that I published on github. 
I developed this during my role as a mentor for a robotics team.  The repo 
uses [the readme](https://github.com/pluteski/first2017mp/blob/master/README.md) to explain the project at a high level manner while also 
giving a developer a quick start guide. 
Often overlooked, [the repo wiki](https://github.com/pluteski/first2017mp/wiki)
is a great way to elaborate further in breadth as well as depth.

The wiki is a great place to provide visuals that allow readers to quickly get a sense of what your project does. 
[Figure 1](https://github.com/pluteski/first2017mp/raw/master/images/stage3_paths_ne.png) 
and [Figure 2](https://github.com/pluteski/first2017mp/raw/master/images/stage2_fleur_de_lis.png) 
are two of the most visually appealing visualizations that give the reader an idea at a glance of what is being done and how, and draw the reader in to learn more.

<p align="center">
<img src="https://github.com/pluteski/first2017mp/raw/master/images/stage2_fleur_de_lis.png" alt="Fleur-de-lis search pattern (copyright Mark E Plutowski)" width="300px"/>
<br>
<b>Figure 1. Fleur-de-lis search pattern </b>
</p>

Figure 1 shows a fleur-de-lis search pattern used by an algorithm that I developed for a simple offline geometric planner. Not only does it illustrate a key step, it is also an appealing visualization. 

<p align="center">
<img src="https://github.com/pluteski/first2017mp/raw/master/images/stage3_paths_ne.png" alt="Sample trajectories (copyright Mark E Plutowski)" width="400px"/>
<br>
<b>Figure 2 sample trajectories found by the planner</b>
</p>

Figure 2 shows trajectories discovered by the planner for one of the goal states overlaid on a depiction of the playing field. 

## Displaying depth and breadth
Whereas a resume doesn't give much space for displaying the full extent of 
your competencies, this format can be used to touch on whatever you want to emphasize.  

You can use the [readme page](https://github.com/pluteski/first2017mp/blob/master/README.md) or [the wiki home page](https://github.com/pluteski/first2017mp/wiki) to link to other pages that provide context for what you were trying to accomplish, and why your approach is sensible. 

For example, the [Background](https://github.com/pluteski/first2017mp/wiki/Background) page of this repo gives a brief primer on Alternative Approaches, and Implementation Steps. The next sections explain the why, what, and how of the project. You could use something similar to demonstrate your depth of understanding of specific techniques. The [Related Approaches page](https://github.com/pluteski/first2017mp/wiki/RelatedApproaches) 
of the repo gives a brief primer on specific alternative approaches that were considered, which one was selected, and why. You could use this to demonstrate your breadth of understanding of a field of interest.  The [Planning Stages page](https://github.com/pluteski/first2017mp/wiki/Planning-Stages) describes what the code actually does.  It gives links to python notebooks containing visualizations, such as [this sample trajectories notebook](https://github.com/pluteski/first2017mp/blob/master/sample_trajectories.ipynb). This allows an interviewer to review your code and see how it behaves on actual data. Notebooks allow you to blend code and visuals and tell the story behind your code without cluttering up the code itself. The [test page](https://github.com/pluteski/first2017mp/blob/master/test.ipynb) gives data from tests designed to simulate the code under realistic conditions and demonstrate its use of compute resources. This type of presentation demonstrates a data-driven mindset and indicates that you know how to test your code rigorously. 

## Customize to your needs

You wouldn’t necessarily need all of these pages.  This serves to illustrate the various types of information that may can be shown, and which are useful for showcasing your competencies.

Many readers may not delve this deep, but for those that do are able to glean more information about your qualifications. You can also refer back to this during the interview. 

Using these assets an interviewer could explore multiple aspects of your skill as a developer, such as your ability to do any of the following :

* Decompose a complex problem
* Design a multifacted system
* Set up a new code repository
* Organize a code repository
* Write performant code
* Visualize key results
* Communicate effectively 

As already stated but which bears repeating, this eases the burden on the interviewer.  Providing these assets gives the interviewer opportunity to derive behavioral questions. It conversely gives you ample content you can use to answer those behavioral questions. Use the STAR technique, by describing the situation (**S**), distilling it down to a single task (**T**), and the key action (**A**) you used to solve it, concluding with the result (**R**). 

It also provides code screeners some code examples that can be used as a starting point for more in depth exploration of your skills. Instead of selecting a random problem for the coding challenge, they may choose one that is within your wheelhouse. 

## Showcasing your repo

Getting off and running is greatly simplified by reviewing other code repos. Review code repositories relevant to your own goals and learn how to distinguish effective ones from half-baked ones. Pick one or two to serve as a role model for your own design, and then make it your own.

## Choosing a coding project 

Many new developers make the mistake of picking code projects similar to what they encountered in an academic setting. These are often not relevant to a real-world job as a commercial software developer, because they exclude some of the messier aspects of a commercial application, such as cleansing a dataset, putting data into a form that can be utilized by an efficient algorithm, and analyzing complex results. Interviewers want to see that you possess knowledge and can learn quickly, but even better if you are familiar with heterogeneous aspects of a development task. A well known AI researcher once said that in the university setting they spent 20% of their time prepping the dataset and 80% on the algorithm development, whereas in a commercial setting this ratio was reversed.

## Choose well
Your mission is to showcase your ability to apply Apache Spark to a nontrivial dataset. As a general rule of thumb -- if you didn’t need to perform any data cleansing or preprocessing on the dataset, the data wasn’t messy enough to be realistic.  If your solution ran in a few seconds it was not challenging enough to test your algorithms. If it didn’t exhaust at least half of the memory available to your system and run the fan to cool off the cpu, then you probably could increase the complexity of the task, either by tackling a larger dataset, or by taking on a more challenging problem based on the dataset.

The key to choosing a code project to show in your portfolio is authenticity. 
If this is not a true representation of what you can do, it can backfire. 
You want the reader to know that this is an original creation of your own making. 

# Ideas for a coding project 

## Migrate an existing solution to Spark 
In this case, there might already be an existing solution for solving a problem.  Your mission is to migrate the solution to Apache Spark and compare the results. Many such datasets and associated solution exist online. 

## Use Spark to analyze a nontrivial data set
By nontrivial here I mean something that takes more than a few seconds to process.  If the data set does not push the compute resources of your development environment you might use an inefficient solution instead of a performant one.  If you use a nontrivial dataset you can showcase the difference between an inefficient algorithm and a performant algorithm.  It also makes the results more interesting, and gives you more to talk about. 

My Apache Spark SQL course on Thinkific showcases a code project based around a 6.5 MiB dataset containing 1,095,695 words, 128,467 lines, and 41,762 distinct words.  The analyses it uses are especially customized for this dataset -- the analyses you choose for the dataset of your choice also indicate your ability to pose interesting questions, create queries for answering those questions, and efficiently implementing those queries on a large dataset. 

## Compare Spark with an alternative computing platform
In this approach, you select a data set, perform an analysis of it using two different programming languages or computing platforms. For example you might first ue scikit-learn, numpy, or pandas, and then do the same analysis using Apache Spark.  Or, you might compare and contrast Hive vs Spark. 

I had performed a similar exercise myself (2016). A couple years later, Databricks published the results of a similar study on their blog, Benchmarking Apache Spark on a Single Node Machine - The Databricks Blog. 
Use Spark along with a cloud api 
Cloud apis provide powerful means of handling large datasets for certain applications; however, preparing the data for upload to the cloud api may require substantial preprocessing. Cloud apis can also generate a lot of log data.  For example, I  evaluated IBM Watson and the Google Cloud speech-to-text cloud apis, and then compared the results (cf., on bleu scores and transcription rates, and on transcription rate for noisy recordings) by analyzing the log data.  In this case, I used sqlite to run the queries. When I redo this I plan to use Spark SQL instead. 

This type of project gives you even more to talk about: how to integrate Spark with a cloud api, what operations are suitable for Spark and which ones are more suitable to do within the cloud api, what post-processing analytics steps are there for which Spark is especially suitable.  Provide visuals where meaningful, e.g. Figure 5.  Your wiki can also show resource consumption by screenshotting the Spark UI, such as shown in Figure 0.



Source: https://learn-apache-spark.thinkific.com/courses/spark-sql 

Figure 3. Frequent 5-tuple analysis on The Collected Works of Sherlock Holmes corpus


## Use Spark to extract training features from a data set
Many data science jobs require the ability to train statistical models based on feature data gleaned from raw data. There is an abundance of data that you could use to demonstrate your ability to perform this. 

My Apache Spark SQL course shows how to extract moving-window n-tuples from a text corpus. Figure 4 illustrates this for 4-tuples, though it is easily generalizable to arbitrary length n-tuples. This would provide a good starting point for a feature extractor that vectorizes this into a form that can be provided as input to a neural network model. 

**Examples of modeling tasks**

* Statistically improbable phrases
* Anomaly detection 
* Topic modeling
* Recommender
* Trend analysis

**Examples of relevant models**

* Approximate K Nearest Neighbors
* Alternating Least Squares
* K-means clustering
* Streaming

If you can generate features for one of these types of models or tasks start-to-finish end-to-end, you are probably able to handle other modeling tasks.




Source: https://learn-apache-spark.thinkific.com/courses/spark-sql 

Figure 4. Moving n-tuple features from The Collected Works of Sherlock Holmes corpus



Source: https://github.com/pluteski/speech-to-text/blob/master/images/bleu_score_deciles.png?raw=true

Figure 5. Bleu score analysis comparing speech-to-text cloud apis


# Conclusion
There are many ways to learn Apache Spark and apply it realistically to nontrivial datasets; however, when it comes to interviewing it still comes down to communicating your understanding effectively.  You can accomplish much of this up-front in advance, while sharpening your skills, by tackling a code project, publishing it, and writing up key results in a  visually appealing way.  

To learn more and see additional tips and project ideas, see my insanely low-priced online courseware on  
[Learning Apache Spark SQL](https://learn-apache-spark.thinkific.com/courses/spark-sql) and 
[Learning Apache Spark ML](https://learn-apache-spark.thinkific.com/courses/sparkml-features)


<-- call the Markdown Preview command via the Command Palette: 

control+shift+p to call up the Command Palette, 
type "Preview", 
select "Markdown Preview: Preview in Browser", and 
hit enter 

option-CMD-O : OmniMarkupPreviewer

-->
