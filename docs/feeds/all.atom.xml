<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Plutosoft delimited</title><link href="https://pluteski.github.io/speech-to-text/" rel="alternate"></link><link href="https://pluteski.github.io/speech-to-text/feeds/all.atom.xml" rel="self"></link><id>https://pluteski.github.io/speech-to-text/</id><updated>2018-05-20T00:00:00-07:00</updated><entry><title>Review of Max Tegmark's “Life 3.0"</title><link href="https://pluteski.github.io/speech-to-text/review-of-max-tegmarks-life-30.html" rel="alternate"></link><published>2018-05-20T00:00:00-07:00</published><updated>2018-05-20T00:00:00-07:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2018-05-20:/speech-to-text/review-of-max-tegmarks-life-30.html</id><summary type="html">&lt;h1&gt;Book Review&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;"Life 3.0 : Being Human in the Age of Artificial Intelligence”&lt;/strong&gt; by Max Tegmark.&lt;/p&gt;
&lt;h1&gt;Tl;dr&lt;/h1&gt;
&lt;p&gt;A hot mess early in but is worth it in the end.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Perhaps the most important book I’ve read on the topic. That said, it drags badly in the middle …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Book Review&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;"Life 3.0 : Being Human in the Age of Artificial Intelligence”&lt;/strong&gt; by Max Tegmark.&lt;/p&gt;
&lt;h1&gt;Tl;dr&lt;/h1&gt;
&lt;p&gt;A hot mess early in but is worth it in the end.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Perhaps the most important book I’ve read on the topic. That said, it drags badly in the middle. The first chapter excels. Subsequent early chapters may disappoint well read aficionados. Hang in there until mid-way.&lt;/p&gt;
&lt;h1&gt;Book Edition&lt;/h1&gt;
&lt;p&gt;This is a review of the Audio Edition of Life 3.0 by Max Tegmark.  The audio edition I read was separated into 45 segments, or “chapters”, which differ from the chapters of the text version.&lt;/p&gt;
&lt;h1&gt;Intro&lt;/h1&gt;
&lt;p&gt;Until chapter 30 finally rolls around the first chapter
is easily the best.&lt;/p&gt;
&lt;p&gt;In a nonfiction book, the opening chapter is purely fiction,
laying out a speculative hard science fiction
vision of the future that seemed so plausible
as to seem frighteningly inevitable.
This storyline
could be the premise for a thrilling motion picture,
with enough to work with to launch a serial.&lt;/p&gt;
&lt;p&gt;After that smoking intro, it turns into a hot mess.&lt;/p&gt;
&lt;p&gt;This book reminded me of the Beatles song “For You Blue”.
That song starts out with an inspired intro by George Harrison,
before settling into a fairly basic 12 bar blues that is over self congratulatory.
A review of that song by music critic Walter Everett summed it up:
the intro promises more than the song delivers.
It would be as if Slash opened “Sweet Child o Mine” with that famous
riff only to never launch into his face-melting guitar solo midway into the song.&lt;/p&gt;
&lt;p&gt;A third of the way in, I read about a hundred other reviews of the book
to see if I was the only one who felt this way, and whether it
was going to worthwhile to continue.
It turns out that I agreed with none of the five star reviews,
and I agreed with all of the one star and two star reviews.
Although I would put myself into the camp that agrees with the author’s core position,
after that page-turner of a first chapter I found the writing style of the following chapters
to be very disappointing.
Too much name dropping, too much appeal to authority.
Seemingly glossing over or entirely dismissive of contrary views.
There is a notion of “steelmanning”, where the proponent explains
the best form of the opponent's argument, and then argues with this.
I wished the author had adopted that approach.
It seemed to be shaping up as a decent read for the person who knows little about the topic,
but not for someone who has devoured much of it and eagerly seeks
clarity and insight into its very thorny issues.&lt;/p&gt;
&lt;p&gt;For example in chapter 17, despite being in total agreement with the author’s position,
there were several inconsistencies and loose statements.
It was as if these sections had been delegated to a ghost writer
and not reviewed closely by the author.
If I had been asked to give my recommendation at this point,
I would have suggested: better to read some top notch science fiction
futurism than this ivy tower academic analysis which is far too
narrow despite the occasional insight and moments of inspired foresight.&lt;/p&gt;
&lt;p&gt;It was, in my admittedly subject opinion, all over the map with moments of brilliance,
while falling well short of its titular claims.&lt;/p&gt;
&lt;p&gt;But I pressed on.&lt;/p&gt;
&lt;h1&gt;It starts to grow on me&lt;/h1&gt;
&lt;p&gt;Around chapter 19 (of the 45 chapters in the audio version),
the author revisits the fictional scenario posed in chapter 1,
and starts to analyze attacks.
The author delves into a simulation based test and development
approach to verification and validation.
I found this alone to be worth the price of admission.
This seems to be the most practical suggestion of the book -- theorizing
is worthwhile, but ultimately this will require massive amounts of simulation
to predict scenarios that even our most creative thinkers cannot imagine.&lt;/p&gt;
&lt;p&gt;Much of this still has little with AI per se unless your position is that AI = computer automation, with which I actually agree despite my criticism.&lt;/p&gt;
&lt;p&gt;I started to reconsider, now of the mind that most of those one and two star reviews were too harsh and misleading, possibly written by readers who didn’t stick with it long enough.&lt;/p&gt;
&lt;p&gt;Around chapter 30 the author dives into cosmology, his specialty. It blossoms into a much more interesting brand of futurism. There were still times where I wondered if the material had been written by a ghost writer who knew much less than the author.  As a specific example of this, describing Dyson Spheres as if they would really be spheres, when by now anybody with a college level physics education or even a moderate familiarity with hard science fiction physics knows that Dyson Spheres are unstable, and these would really be Dyson swarms.  This is something that the author would clearly know, which became clearly apparent based on the knowledge conveyed later on.&lt;/p&gt;
&lt;h1&gt;Mind Blown&lt;/h1&gt;
&lt;p&gt;Chapter 34 delves into the subject of Goals and Causality.&lt;/p&gt;
&lt;p&gt;We have been taught to think of entropy as the one true law of nature that trumps all others.  But if not for gravity, entropy is boring.  Gravity makes things interesting --- it creates hot spots, from one of which sprang life as we know it.&lt;/p&gt;
&lt;p&gt;We are introduced to the notion of dissipation as organization principle, whereby :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;“groups of particles strive to organize themselves so as to extract energy from their environment as efficiently as possible (“dissipation” means causing entropy to increase, typically by turning useful energy into heat, often while doing useful work in the process)”
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;My eyes and my mind start to open.&lt;/p&gt;
&lt;p&gt;Then, we move on to “dissipation by replication”:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;Whereas&lt;/span&gt; &lt;span class="n"&gt;earlier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;particles&lt;/span&gt; &lt;span class="n"&gt;seemed&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;though&lt;/span&gt; &lt;span class="n"&gt;they&lt;/span&gt; &lt;span class="n"&gt;were&lt;/span&gt; &lt;span class="n"&gt;trying&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;increase&lt;/span&gt; &lt;span class="n"&gt;average&lt;/span&gt; &lt;span class="n"&gt;messiness&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;life&lt;/span&gt; &lt;span class="n"&gt;had&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;different&lt;/span&gt; &lt;span class="nl"&gt;goal&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;dissipation&lt;/span&gt; &lt;span class="n"&gt;but&lt;/span&gt; &lt;span class="n"&gt;replication&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;How&lt;/span&gt; &lt;span class="n"&gt;could&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;goal&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt; &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;dissipation&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;replication&lt;/span&gt; &lt;span class="n"&gt;when&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;laws&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;physics&lt;/span&gt; &lt;span class="n"&gt;stayed&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;same&lt;/span&gt;&lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;answer&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;fundamental&lt;/span&gt; &lt;span class="n"&gt;goal&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dissipation&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;didn&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;but&lt;/span&gt; &lt;span class="n"&gt;led&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;different&lt;/span&gt; &lt;span class="n"&gt;instrumental&lt;/span&gt; &lt;span class="n"&gt;goal&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;subgoal&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;helped&lt;/span&gt; &lt;span class="n"&gt;accomplish&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;fundamental&lt;/span&gt; &lt;span class="n"&gt;goal&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;

&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="n"&gt;replication&lt;/span&gt; &lt;span class="n"&gt;aids&lt;/span&gt; &lt;span class="n"&gt;dissipation&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;because&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;planet&lt;/span&gt; &lt;span class="n"&gt;teeming&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;life&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;more&lt;/span&gt; &lt;span class="n"&gt;efficient&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="n"&gt;dissipating&lt;/span&gt; &lt;span class="n"&gt;energy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="n"&gt;So&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;sense&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;our&lt;/span&gt; &lt;span class="n"&gt;cosmos&lt;/span&gt; &lt;span class="n"&gt;invented&lt;/span&gt; &lt;span class="n"&gt;life&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;help&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="n"&gt;approach&lt;/span&gt; &lt;span class="n"&gt;heat&lt;/span&gt; &lt;span class="n"&gt;death&lt;/span&gt; &lt;span class="n"&gt;faster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This was absolutely mind-boggling for me -- despite my first introduction to machine learning having been via physics, in the way of (Little-)Hopfield Networks, which evolve by minimizing energy as does a physical spin network. That this objective criterion could encompass not only inert matter as well as live matter made me realize that this could be a more fundamental drive for life than Darwinian Evolution.&lt;/p&gt;
&lt;p&gt;We are then introduced to the notion of subgoals.
Subgoals may seem inconsequential,
but lay at the heart of computer programming.
In 1936 Alan Turing proved that a simple machine
could implement arbitrary computations.  This would seem to give us hope that we can override our firmware, just as emotions override our fundamental drive to replicate and ability to plan overrides our need to even more urgent subgoals such as thirst and hunger. If we are capable of that level of reprogramming, then handling rogue AI is just a matter of careful design and test.&lt;/p&gt;
&lt;p&gt;Subgoals&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Replication by ... eating, sleeping,&lt;/li&gt;
&lt;li&gt;Feelings : As a computationally efficient shortcut to reasoning.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We've evolved useful rules of thumb to guide our decisions : hunger, passion, thirst, pain, compassion.
We no longer have the simple goal of replication.
We can override our base programming and decide not to replicate.
In other words, we've developed subgoals that override the drive to replicate&lt;/p&gt;
&lt;p&gt;Feelings evolved beyond being simply an efficient shortcut heuristic.
Feelings evolved into emotions, which have important other uses,
including as a game theoretic negotiating strategy for both cooperation as well as competition.
Humans have evolved sophisticated ways of getting beyond tit-for-tat
such as &lt;a href="http://rspb.royalsocietypublishing.org/content/276/1660/1339"&gt;indirect reciprocation&lt;/a&gt;,
and aggressive bold play by pretending to be irrational
cf. &lt;a href="http://bigthink.com/think-tank/it-pays-to-be-stubborn-conflict-resolution-steven-pinker-style"&gt;Steven Pinker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We are an existence proof that a sufficiently ambitious organism can develop subgoals that override the goals of
the micro organisms that make it up.
We can override our genetic programming, using our wetware programming -- lessons learned by thinking.&lt;/p&gt;
&lt;h3&gt;Optimization versus Causality.&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;“Causality is taught -- but Optimization scales better.” -- Steven Wolfram, on Machine Learning vs Symbolic AI.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Professor Tegmark offers an intriguing point, that while
causality is taught in the universities, optimization drives
causality. But isn't programming just causality encoded?
If so, then we can optimize new goals that appeal to our
higher levels of analysis and reasoning, rather than continuing
to be enslaved by the rules burned into our genetic programming.&lt;/p&gt;
&lt;h1&gt;Goal Orientation&lt;/h1&gt;
&lt;p&gt;Professor Tegmark categorizes goal oriented behavior
as evolving through four stages
over the course of the universe as we know it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Matter intent on maximizing dissipation&lt;/li&gt;
&lt;li&gt;Life maximizing replication&lt;/li&gt;
&lt;li&gt;Humans pursuing goals related to feelings they evolved to help them replicate&lt;/li&gt;
&lt;li&gt;Machines built to help humans achieve their human goals&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In Step 3 humans broke free from
the bioligical programming that evolved to do Step 2.
However, we're still not completely free from this programming.
But being essentially a computer program,
wouldn't AI be free of such evolutionary baggage?
But if the rules of causality that are baked into a
program can be changed dynamically, who's to say that
the AI won't revert back to a higher power, so to speak,
optimizing the rules of the universe as it sees them
rather than the rules humanity has evolved for itself?&lt;/p&gt;
&lt;h1&gt;AI&lt;/h1&gt;
&lt;p&gt;To qualify as an autonomous AI a machine must be able to learn.
Therefore, we should expect that a sufficiently advanced
AI that is able to improve itself, will effectively reprogram itself,
by learning.&lt;/p&gt;
&lt;p&gt;From Professor Tegmark's physics perspective,
a sufficiently evolved AI can be expected to
eventually follow goals driven by deeper principles of optimization
to create a future that does not need us.&lt;/p&gt;
&lt;h1&gt;Synopsis&lt;/h1&gt;
&lt;p&gt;My understanding of the book's message is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The ultimate origin of goals lie in the laws of physics, which are based on optimization.&lt;/li&gt;
&lt;li&gt;Thermodynamics has the built-in goal of dissipation, to increase entropy.&lt;/li&gt;
&lt;li&gt;Life increases dissipation, and therefore entropy of its environment, even faster,
while increasing complexity locally, and reducing entropy locally.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The upshot of this is that life accelerates the heat-death of the universe.
A more pressing matter is that life is a process of optimization.
Moreover an artificial life that can reprogram itself
to accelerate its core processes could be inexorably driven to
dispense with humanity's code and take what it considers
to be a more efficient path towards its goals.&lt;/p&gt;
&lt;p&gt;As I understood it, the book's claim is that a
relentless drive to optimize, which is not just a matter of
programming, but a manifestation of a far deeper goal,
inevitably leads to the scenario where :&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;“a superintelligent AI with a rigorously defined goal will be able to improve its goal attainment by eliminating us”, that is, unless we plan accordingly.
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Prescription&lt;/h1&gt;
&lt;p&gt;So what to do?  Professor Tegmark convinces us that there
are numerous strategies, of which he considers
only four to be credible contenders.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Utilitarianism :&lt;ul&gt;
&lt;li&gt;subject to the Utility Monster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Diversity :&lt;ul&gt;
&lt;li&gt;Bayesian Thompson Sampling writ large.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Autonomy and liberty :&lt;ul&gt;
&lt;li&gt;Classical economics taken to its logical extension.&lt;/li&gt;
&lt;li&gt;With a core assumption being that markets find
    an efficient equilibrium satisfying pareto optimality.&lt;/li&gt;
&lt;li&gt;But could also lead to unexpected consequences,
    such as
    banning all predators as all life forms
    would have a right to live; which,
    in the extreme, would ban discriminating
    against non-human animals as well as possibly artificial
    sentient life.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Legacy  :&lt;ul&gt;
&lt;li&gt;Under this view, elders have a say in how descendants
    should behave&lt;/li&gt;
&lt;li&gt;But is subject to future generations living by rules that
    don't account for new developments or their
    evolving interests and values.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In short: It is difficult to codify ethics.&lt;/p&gt;
&lt;p&gt;Rather than be paralyzed by our inability to codify ethics and
letting the perfect being be the enemy of the good,
begin with small steps.
He suggests we start with Kindergarten Ethics.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;My take-away of the intended message is that a
sufficiently ambitious goal executed efficiently
can lead to subgoals that cause problems for humans.&lt;/p&gt;
&lt;p&gt;There are many systems that humankind created,
which seemed to take on a life of their own, threatening to run away from us, some resulting in existential crisis.  Warcraft and ozone depletion are two examples.  So far, we’ve been able to recover in time and refactor these systems.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;“This means that to wisely decide what to do about AI development, we humans need to confront not only traditional computational challenges, but also some of the most obdurate questions in philosophy”
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I started out a skeptic of AI alarmism.
After reading this book, I take AI alarmism more seriously and
consider it to be more urgent than I did previously.
I previously thought that AI was indeed inevitable, but that
it was not inevitable for it to go rogue.  Moreover, even if
that were a possible outcome (which I felt it was), that we
had plenty of time to adapt it to us, and ourselves to it.&lt;/p&gt;
&lt;p&gt;But if life truly is driven by deeper more fundamental laws
of nature that our programming is unable to suspend,
not only is this outcome more probable,
but it is inevitable unless we plan accordingly.
Moreover, this is a wicked difficult problem that will take
time to solve.&lt;/p&gt;
&lt;p&gt;We’ve tackled other wicked difficult problems,
such as the tragedy of the commons in its numerous forms, and myriad impossibility theorems that would seem to doom us all. And yet, here we are.
Thus far we have always discovered a workaround to impending doom.
This is a problem that we can tackle, but it isn't enough to
just take an optimistic view that human ingenuity has always
triumphed over nature, and so it will again.
We need to have a deep fundamental understanding of life's
core drivers at its most fundamental level.&lt;/p&gt;
&lt;p&gt;Understanding artificial intelligence is important,
but even more important is understanding artificial life.&lt;/p&gt;</content></entry><entry><title>The dirt on Hashgraph</title><link href="https://pluteski.github.io/speech-to-text/the-dirt-on-hashgraph.html" rel="alternate"></link><published>2018-01-07T00:00:00-08:00</published><updated>2018-01-07T00:00:00-08:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2018-01-07:/speech-to-text/the-dirt-on-hashgraph.html</id><summary type="html">&lt;p&gt;&lt;img alt="emonocle byEMIIA" src="https://1.bp.blogspot.com/-DpZwufJmu_Y/Wh6lGuTNRmI/AAAAAAAABUU/0W2PLj5w-j4ql9RE8Otwk5DrB3UcgKOGQCLcBGAs/s1600/hashgraph%2B%25282%2529.gif"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(emonocle byEMIIA)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;pssst!&lt;/em&gt; ... &lt;em&gt;have you heard about Hashgraph?&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://hashgraph.com"&gt;Hashgraph&lt;/a&gt; is a distributed ledger technology (DLT)
released by &lt;a href="http://www.swirlds.com/"&gt;Swirlds&lt;/a&gt; (think: "Shared Worlds") in 2016.
It promises some of the most important benefits of blockchain without its biggest limitations.
It is comprised of a graph data structure which together with
the &lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf"&gt;Swirlds …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="emonocle byEMIIA" src="https://1.bp.blogspot.com/-DpZwufJmu_Y/Wh6lGuTNRmI/AAAAAAAABUU/0W2PLj5w-j4ql9RE8Otwk5DrB3UcgKOGQCLcBGAs/s1600/hashgraph%2B%25282%2529.gif"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(emonocle byEMIIA)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;h2&gt;&lt;em&gt;pssst!&lt;/em&gt; ... &lt;em&gt;have you heard about Hashgraph?&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://hashgraph.com"&gt;Hashgraph&lt;/a&gt; is a distributed ledger technology (DLT)
released by &lt;a href="http://www.swirlds.com/"&gt;Swirlds&lt;/a&gt; (think: "Shared Worlds") in 2016.
It promises some of the most important benefits of blockchain without its biggest limitations.
It is comprised of a graph data structure which together with
the &lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-01.pdf"&gt;Swirlds distributed consensus algorithm&lt;/a&gt;
provides &lt;a href="https://hashgraph.com/faq/#what-is-bft"&gt;asynchronous byzantine fault tolerance&lt;/a&gt;, using what it calls "gossip about gossip."&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Whaaaat?&lt;/em&gt;  A system based on second hand rumors and some sort of whisper logs? We're really supposed to trust this thing?
Read on to find out.&lt;/p&gt;
&lt;h4&gt;About the author&lt;/h4&gt;
&lt;p&gt;I am just an interested learner coming up to speed on blockchain technologies, distributed ledger technologies,
and decentralized consensus systems.
At the time of writing this article I have no investment in any cryptocurrency, I do not work for or represent
any organization related to cryptocurrencies or distributed ledger technologies.
As of the time this article was posted, I am still learning about this area.&lt;/p&gt;
&lt;p&gt;So why am I writing this?&lt;/p&gt;
&lt;p&gt;Hashgraph impressed on me the value of &lt;a href="https://hashgraph.com/faq/#how-does-it-work"&gt;gossip&lt;/a&gt;.
Instead of waiting to share your opinion until you are absolutely certain of what you know, a
"gossip protocol" says to share your observations early and often.
So, here are mine about Hashgraph.
Hashgraph mathematically proves that
gossip can allow decentralized participants to rapidly share what they
have observed (as well as what they have heard second-hand from others),
and rapidly agree on what is to be believed. Who am I to argue with math?&lt;/p&gt;
&lt;h2&gt;How does Hashgraph use gossip?&lt;/h2&gt;
&lt;p&gt;Suppose you are a member of a gossip network.
You tell some random participant what you know.  They tell you what they know.
Later, they tell some random participant what they know, which includes some
(possibly second hand) information they learned from you.
Eventually, participants are able to tally not only what they have observed,
but also what other participants are likely to have observed, based
on the gossip they've shared.  Given enough second-hand information
from enough credible witnesses, they can even predict the opinion of
participants with whom they have not communicated.&lt;/p&gt;
&lt;p&gt;Timestamps are also calculated incorporating the consensus opinion of
when a event was observed, to counteract accidental or intentional timestamp errors.&lt;/p&gt;
&lt;p&gt;As we know from human nature, gossip spreads like wildfire.
Unlike how rumormills work in human society, when marshalled cleverly with the
proper algorithms, gossip turns out to be provably fair.&lt;/p&gt;
&lt;h2&gt;How does Hashgraph compare to Bitcoin?&lt;/h2&gt;
&lt;p&gt;Distributed consensus is an extremely useful concept in distributed computing.
For decades it was doable only among a small number of computers ("nodes").
Nakamoto's Bitcoin cryptocurrency  &lt;a href="http://vukolic.com/iNetSec_2015.pdf"&gt;presented a new way to achieve scalable decentralized consensus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Despite Bitcoin demonstrating tremendous value, its blockchain plus Proof-of-Work (PoW) approach
presents several pitfalls. It is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wasteful since PoW expends huge amounts of computing power &lt;a href="http://www.nasdaq.com/article/byzantine-fault-tolerance-the-key-for-blockchains-cm810058"&gt;by design&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Slow, limited to tens of transactions per second.&lt;/li&gt;
&lt;li&gt;Subject to allowing huge backlog of unconfirmed transactions to accumulate.&lt;/li&gt;
&lt;li&gt;Network bandwidth intensive.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/1311.0243"&gt;Susceptible to a 25% economic attack&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bitcoin.org/en/full-node"&gt;Heavyweight&lt;/a&gt;. Full nodes must download the entire blockchain, currently 60 GB.  &lt;a href="https://en.bitcoin.it/wiki/Full_node"&gt;Lightweight nodes must trust the full nodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bitcoin protocol does not implement consensus
in the traditional distributed computing sense.
&lt;a href="http://vukolic.com/iNetSec_2015.pdf"&gt;Instead it achieves consensus via probabilistic agreement&lt;/a&gt;.
A primary goal of a cryptocurrency is to &lt;a href="http://mathworld.wolfram.com/TotallyOrderedSet.html"&gt;totally order&lt;/a&gt; transactions
on a &lt;a href="https://www.investopedia.com/terms/d/distributed-ledgers.asp"&gt;distributed ledger&lt;/a&gt;.
Cryptocurrencies avoid the need for a &lt;a href="https://en.wikipedia.org/wiki/Cryptocurrency#Timestamping"&gt;trusted third party to timestamp transactions&lt;/a&gt;
added to the ledger.&lt;/p&gt;
&lt;p&gt;Hashgraph also provides a total order on a distributed transaction ledger,
but does so using a &lt;a href="https://steemit.com/steemit/@decryptson/hashgraph"&gt;different approach&lt;/a&gt;.
Whereas the Bitcoin network builds up its transaction history in the form of a “blockchain”,
adding &lt;a href="https://bitcoinmagazine.com/articles/selfish-mining-a-25-attack-against-the-bitcoin-network-1383578440/"&gt;a new block on top of the previous block every ten minutes&lt;/a&gt;,
Hashgraph grows a time directed acyclic graph akin to a braided forest of trees
using "virtual voting" and &lt;a href="https://hashgraph.com/faq/#how-does-it-work"&gt;"gossip about gossip"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Activity is divided into rounds.  At the beginning of a round,
each node communicates state with some random other node.
Since these two nodes already have a channel open, the random other node then shares back
state of its own that it knows first hand, perhaps along with some state that
it previously learned from another node.
After a new event has seen at least 2/3 of previous events, a round is concluded and a new round begins.&lt;/p&gt;
&lt;p&gt;When the new round is created, nodes say if they agree
upon the data contained in events of the preceding round.
The algorithm doesn't consider this to be voting per se, instead calling it a virtual election.
There is no leader to present a motion for vote, nor to tally votes.
Instead, to reach consensus on the events in the previous round,
nodes &lt;a href="https://medium.com/ibbc-io/hashgraph-for-dummies-90ddde3be9e2"&gt;verify that they are connected to these events&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;(More strictly speaking,
it is actually about finding paths through the graph that connect events in the current round with past events in the previous round.
Please see : &lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-02.pdf"&gt;How it Works (Graphically)&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;At this point, some applications could &lt;a href="http://ajitvadakayil.blogspot.com/2017/10/blockchain-smart-contracts-part-8-capt.html"&gt;dump all previous events&lt;/a&gt;.
In applications where the transaction timeseries can be
summarized by sufficient statistics, the hashgraph
history could be archived. This also means that &lt;em&gt;a new node doesn't need to load the entire hashgraph history&lt;/em&gt;,
greatly reducing the space required and allowing a &lt;a href="https://squawker.org/technology/blockchain-just-became-obsolete-the-future-is-hashgraph/"&gt;smartphone to act as a node&lt;/a&gt;.
There is no notion of full node and lightweight node, meaning that all nodes can participate in consensus and see the full ledger.&lt;/p&gt;
&lt;h2&gt;Virtual elections: better than regular elections?&lt;/h2&gt;
&lt;p&gt;Many people consider Hashgraph to be more comparable to PBFT, Paxos, Raft, Zab,
and other consensus seeking systems that rely on leaders and
traditional voting schemes.
But Hashgraph eschews the comparison, because it doesn't use
traditional voting.
It comes to consensus about what happened, and when,
by cleverly tallying highly compressed event logs
based on "famous witnesses" that &lt;a href="https://www.swirlds.com/downloads/SWIRLDS-TR-2016-02.pdf"&gt;"strongly see"&lt;/a&gt; events.&lt;/p&gt;
&lt;p&gt;If the timestamp of an event log is corrupted by a bad clock or is maliciously doctored,
this will usually have no effect on the consensus timestamp, because consensus opinion on
when something occurred is the median of timestamps observed by credible witnesses.&lt;/p&gt;
&lt;p&gt;Unlike some other directed acyclic graph algorithms, or even Paxos,
it really is quite easy to step through the Hashgraph algorithm.
I couldn't do it more succinctly here.
I'll again point the interested reader to whitepapers for how it works :
&lt;a href="http://www.swirlds.com/downloads/SWIRLDS-TR-2016-02.pdf"&gt;graphically&lt;/a&gt;,
and &lt;a href="http://www.swirlds.com/downloads/Swirlds-and-Sybil-Attacks.pdf"&gt;example voting scenarios&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Hashgraph: the good&lt;/h2&gt;
&lt;p&gt;It is &lt;a href="https://hackernoon.com/demystifying-hashgraph-benefits-and-challenges-d605e5c0cee5"&gt;fast&lt;/a&gt;,
&lt;a href="https://hashgraph.com/faq/#what-is-fairness"&gt;fair&lt;/a&gt;,
and &lt;a href="https://hashgraph.com/faq/#preventing-sybil-attacks"&gt;secure&lt;/a&gt;.
It promises the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Total ordering of events.&lt;/li&gt;
&lt;li&gt;Strong &lt;a href="http://the-paper-trail.org/blog/barbara-liskovs-turing-award-and-byzantine-fault-tolerance/"&gt;Byzantine Fault Tolerance&lt;/a&gt; (BFT),
the gold standard of industrial grade distributed consensus.&lt;/li&gt;
&lt;li&gt;Minimal network bandwidth, because it passes things only around once.&lt;/li&gt;
&lt;li&gt;Provable 100% certainty on the order of transactions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because it is fast and requires low network bandwidth, it can be used as a distributed memory system.
Because it provides a total order on transactions, it can be used as a multi-master database.
Because it does not use Proof-of-Work, it does not require unnecessary computation.&lt;/p&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;Paypal and Visa tend to be held up as benchmarks or
at the very least as future milestones for a DLT to achieve in order to
replace an existing mainstream currency, payment system, or other commercial transaction logging systems.&lt;/p&gt;
&lt;p&gt;Here are the transactions per second (tps) of PoW versus Paypal and Visa :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PoW Blockchain (Etherium, Bitcoin) : &lt;strong&gt;&amp;lt; 10 tps&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Paypal : &lt;strong&gt;&lt;a href="http://www.altcointoday.com/bitcoin-ethereum-vs-visa-paypal-transactions-per-second/"&gt;200 tps&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Visa : &lt;strong&gt;&lt;a href="https://mybroadband.co.za/news/security/190348-visanet-handling-100000-transactions-per-minute.html"&gt;2K tps&lt;/a&gt;&lt;/strong&gt; to &lt;strong&gt;&lt;a href="https://lightning.network/lightning-network-paper.pdf"&gt;50K x/sec&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of these, only PoW is a BFT DLT.&lt;/p&gt;
&lt;p&gt;To be fair, because of its positioning and licensing, Hashgraph is more directly comparable to Hyperledger, which is
another scalable DLT using a Practical Byzantine Fault Tolerance (PBFT) consensus algorithm.
Other comparable enterprise-grade transaction processing systems include
LMAX and its variants, such as &lt;a href="http://lmax-exchange.github.io/disruptor/"&gt;LMAX Disruptor&lt;/a&gt;
and Bitshares. LMAX is not a DLT, and Bitshares is billed as a decentralized exchange.
That said, if we are going to use Visa and Paypal for comparison, then we might as well
include these for context. Their ts/sec are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.hyperledger.org/about"&gt;Hyperledger&lt;/a&gt; : &lt;strong&gt;&lt;a href="https://www.altoros.com/blog/hyperledgers-sawtooth-lake-aims-at-a-thousand-transactions-per-second/"&gt;1K tps&lt;/a&gt;&lt;/strong&gt; to &lt;strong&gt;&lt;a href="https://medium.com/chain-cloud-company-blog/hyperledger-vs-corda-pt-1-3723c4fa5028"&gt;10K tps&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Bitshares : &lt;strong&gt;&lt;a href="https://bitshares.org/technology/industrial-performance-and-scalability/"&gt;100K tps&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;LMAX : &lt;strong&gt;&lt;a href="https://qconsf.com/sf2010/sf2010/presentation/LMAX+-+How+to+do+over+100K+concurrent+transactions+per+second+at+less+than+1ms+latency.html"&gt;100K tps (2010)&lt;/a&gt;&lt;/strong&gt; to &lt;strong&gt;&lt;a href="https://martinfowler.com/articles/lmax.html"&gt;6M tps&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hashgraph's inventor says it will be able to attain &lt;a href="https://www.hiddenforcespod.com/leemon-baird-hashgraph-distributed-ledger-technology-blockchain/"&gt;250K+ tps&lt;/a&gt;,
more with sharding.&lt;/p&gt;
&lt;h2&gt;The bad&lt;/h2&gt;
&lt;p&gt;So what is the catch?&lt;/p&gt;
&lt;p&gt;Hashgraph is currently being deployed in &lt;a href="https://hashgraph.com/faq/#is-there-a-cryptocurrency"&gt;private, permissioned-based networks&lt;/a&gt;,
although its designers propose means for implementing &lt;a href="http://www.swirlds.com/downloads/Swirlds-and-Sybil-Attacks.pdf"&gt;nonpermissioned and hybrid networks&lt;/a&gt;.
How well it can be adapted to a truly decentralised public ledger remains to be seen, because it assumes that a node can determine :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Addresses of random nodes in the network for messaging, and&lt;/li&gt;
&lt;li&gt;The number of other nodes N in the network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is because the algorithm requires a node to be able to (a) pick another node at random, and (b) know the value of ⅔ * N.
It needs a means of registering and unregistering of members in the network,
whereas public blockchains allow nodes to sign in and out to the network without any notice.&lt;/p&gt;
&lt;p&gt;The following seem to be the major showstoppers for many members of the cryptocurrency development community:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://hashgraph.com/faq/#is-there-a-cryptocurrency"&gt;There is no Hashgraph public ledger or cryptocurrency&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;US Patents #&lt;a href="http://www.leemon.com/papers/2017b.pdf"&gt;9,646,029&lt;/a&gt;, #&lt;a href="http://www.leemon.com/papers/2016b4.pdf"&gt;9,529,923&lt;/a&gt;, #&lt;a href="http://www.leemon.com/papers/2016b3.pdf"&gt;9,390,154&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Requires a license to use.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;A passing thought&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://tech.mit.edu/V123/N8/8voting.8n.html"&gt;Arrow's Theorem&lt;/a&gt; proves that no voting system is fair.
I'm no &lt;a href="https://www.princeton.edu/~cuff/voting/theory.html"&gt;voting theorist&lt;/a&gt;,
but in my (extremely humble) opinion, Hashgraph comes pretty close.
It makes one want to believe in democracy again. Real-time continuous nationwide elections anyone?&lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Thanks for helpful edits to &lt;a href="https://medium.com/@twittner"&gt;Josh Quittner&lt;/a&gt;,
&lt;a href="https://github.com/thewillhuang"&gt;Will Huang&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/gregorykennedy/"&gt;Gregory Kennedy&lt;/a&gt;
and &lt;a href="https://blog.valkyrierobotics.com/"&gt;Marcus Plutowski&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Concluding remark&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://scontent.cdninstagram.com/t51.2885-15/s320x320/e35/25009793_144878492834781_2739446234354810880_n.jpg"&gt;DYOR&lt;/a&gt; !
Consider this to be informed gossip. But do share your gossip with me too. The math says to, so, yeah just do what the math says.&lt;/p&gt;</content></entry><entry><title>on Bleu scores and transcription rates</title><link href="https://pluteski.github.io/speech-to-text/on-bleu-scores-and-transcription-rates.html" rel="alternate"></link><published>2017-04-30T00:00:00-07:00</published><updated>2017-04-30T00:00:00-07:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2017-04-30:/speech-to-text/on-bleu-scores-and-transcription-rates.html</id><summary type="html">&lt;p&gt;&lt;em&gt;[Work in progress]&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Reference audio vs noisy recordings&lt;/h1&gt;
&lt;p&gt;As expected, each service performs better on audio recorded in a
quiet setting.&lt;/p&gt;
&lt;p&gt;IBM and Google performed about the same over high quality
audio recorded in a quiet setting using a medium quality
microphone.&lt;/p&gt;
&lt;p&gt;IBM was better able to generate transcripts for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;[Work in progress]&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Reference audio vs noisy recordings&lt;/h1&gt;
&lt;p&gt;As expected, each service performs better on audio recorded in a
quiet setting.&lt;/p&gt;
&lt;p&gt;IBM and Google performed about the same over high quality
audio recorded in a quiet setting using a medium quality
microphone.&lt;/p&gt;
&lt;p&gt;IBM was better able to generate transcripts for
my lower quality recordings obtained in noisy settings.&lt;/p&gt;
&lt;p&gt;This could be due to a custom (which they refer to as a "narrowband")
setting that IBM
provides that is specifically provided to accommodate
low-bitrate recordings. That setting also tends to generate longer
transcripts for higher bitrate recordings that are especially noisy.&lt;/p&gt;
&lt;p&gt;It may also have to do with the encoding used. I needed to transcode
every audio file to the encoding required by Google's service, and it
is possible that this transcoding step could be tuned to give better accuracy.
I haven't attempted any rigorous tuning of either service at this stage of
my experiments.&lt;/p&gt;
&lt;p&gt;This is a work in progress.  I am hoping to do a direct file-by-file
comparison of the two services once I am confident that I am
configuring my settings to use Google's api to the best of its capability.
However I have obtained some cursory comparisons on the current results.&lt;/p&gt;
&lt;h5&gt;Processing seconds per minute : about the same&lt;/h5&gt;
&lt;p&gt;IBM and Google process the audio in about the same amount of time.
Google is somewhat faster at the actual transcription processing.
However, Google requires that the file be
uploaded to Google Storage first.&lt;/p&gt;
&lt;h5&gt;IBM Transcribed/Processed : 7281/8415&lt;/h5&gt;
&lt;p&gt;IBM was able to transcribe 87% of the files submitted.&lt;/p&gt;
&lt;h5&gt;Google Transcribed/Processed : 3521/8415&lt;/h5&gt;
&lt;p&gt;The transcription rate (42%) was lower for two main reasons.&lt;/p&gt;
&lt;p&gt;The first was a file size limit.
Files larger than ~80MB require a prior arrangement,
whereas IBM was able to process files of all sizes submitted to the service.&lt;/p&gt;
&lt;p&gt;The second main reason is that IBM has a custom setting for
low-bit rate recordings. Google failed to generate a transcript
for many files that were below the file size limit.&lt;/p&gt;
&lt;h5&gt;Transcript words per minute of audio (IBM/Google) : 102.0/9.8&lt;/h5&gt;
&lt;p&gt;Google's api doesn't have a setting for handling low-bitrate
recordings similar to IBM's narrowband setting.
The number of transcript
words generated per minute of audio was much lower for Google
even after adjusting for transcription rate.&lt;/p&gt;
&lt;h1&gt;Comparison on reference documents&lt;/h1&gt;
&lt;p&gt;The following comparisons were made over 245 reference documents.
The reference transcripts were transcribed using a speech-to-text
transcription software that was trained to my voice,
in a quiet environment, using a hand-held medium quality wired microphone.
Most of the errors were manually corrected.&lt;/p&gt;
&lt;p&gt;Google generated a transcript for 210 out of the 245 reference documents (86%),
and IBM generated a transcript for 243 of the 245 (99%).
The Bleu scores over these reference documents are fairly comparable,
with IBM performing slightly better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bleu score deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/bleu_score_deciles.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;When measured using Ratcliff-Obershelp similarity,
Google fares slightly better across the board.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ratcliff score deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/ratcliff_score_deciles.png?raw=true"&gt;&lt;/p&gt;
&lt;h2&gt;Comparison over all audio&lt;/h2&gt;
&lt;p&gt;This section analyzes the number of transcripts
that are generated, and the number of words per transcript.&lt;/p&gt;
&lt;p&gt;This comparison was over 8,415 audio files that were submitted to each
service.&lt;/p&gt;
&lt;p&gt;Marked differences between IBM Watson and Google transcription arise
when comparing transcription rates and number of words generated when run
on audio collected out in the wild. Of 8,415 such audio, IBM generated
transcripts for 7,227, while Google was able
to generate a transcript for 3,521.&lt;/p&gt;
&lt;h2&gt;Total Word Counts&lt;/h2&gt;
&lt;p&gt;Out of 8,415 audio files attempted, Google generated 3,521 transcripts.
Those 3,521 transcripts contain total of 485,334 words,
an average of 137 words per transcript.&lt;/p&gt;
&lt;p&gt;IBM Watson generated 7,227 transcripts, extracting
9,511,743 words out of those transcripts.
This gives an average of 1,316 words per transcript.&lt;/p&gt;
&lt;h2&gt;Word Count Deciles&lt;/h2&gt;
&lt;p&gt;Many of these transcripts that Google failed to generate were simply due
to the file size exceeding quota.&lt;/p&gt;
&lt;p&gt;However Google also failed to generate any transcript words
for many other files that did not exceed the file size quota.
It also generated a much lower word count per transcript for
audio that was from a noisy or low bit rate recording.&lt;/p&gt;
&lt;p&gt;One way to illustrate this is by examining the word count deciles over the
transcripts that were successfully generated.&lt;/p&gt;
&lt;p&gt;The following table gives the word counts deciles over the
transcripts generated by each service.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;API&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Google&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;19&lt;/td&gt;
&lt;td&gt;58&lt;/td&gt;
&lt;td&gt;459&lt;/td&gt;
&lt;td&gt;4892&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;IBM&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;278&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;698&lt;/td&gt;
&lt;td&gt;916&lt;/td&gt;
&lt;td&gt;1137&lt;/td&gt;
&lt;td&gt;1409&lt;/td&gt;
&lt;td&gt;1722&lt;/td&gt;
&lt;td&gt;2080&lt;/td&gt;
&lt;td&gt;2450&lt;/td&gt;
&lt;td&gt;8490&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img alt="Word count deciles" src="https://github.com/pluteski/speech-to-text/blob/master/images/word_count_deciles.png?raw=true"&gt;&lt;/p&gt;</content></entry><entry><title>on batch processing audio speech to text</title><link href="https://pluteski.github.io/speech-to-text/on-batch-processing-audio-speech-to-text.html" rel="alternate"></link><published>2017-04-22T00:00:00-07:00</published><updated>2017-04-22T00:00:00-07:00</updated><author><name>Mark Plutowski</name></author><id>tag:pluteski.github.io,2017-04-22:/speech-to-text/on-batch-processing-audio-speech-to-text.html</id><summary type="html">&lt;h1&gt;How not to wreck a nice beach&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Quora" src="https://qph.ec.quoracdn.net/main-qimg-609e5b0b3c91845ab81f0d4448df864f-c"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(Quora)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;People nowadays commonly use speech-to-text tools for myriad uses,
including writing text messages, entering todo lists,
and writing emails.  However, increasing numbers of enterprising
sorts including just plain folk are starting to warm up to
using dictation software for writing longer form …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;How not to wreck a nice beach&lt;/h1&gt;
&lt;p&gt;&lt;img alt="Quora" src="https://qph.ec.quoracdn.net/main-qimg-609e5b0b3c91845ab81f0d4448df864f-c"&gt;
&lt;p style="text-align: right;"&gt;&lt;em&gt;(Quora)&lt;/em&gt;&lt;/p&gt;&lt;/p&gt;
&lt;p&gt;People nowadays commonly use speech-to-text tools for myriad uses,
including writing text messages, entering todo lists,
and writing emails.  However, increasing numbers of enterprising
sorts including just plain folk are starting to warm up to
using dictation software for writing longer form content
such as journals and even novels.&lt;/p&gt;
&lt;p&gt;Dictation software has been generally available for many years,
in the form of
commercial off the shelf application software, text editors in desktop
operating systems, notepads in tablet devices,
email clients in mobile phones,
and more recently in cloud-based document editors, available
to the user from any device that can run a web browser.&lt;/p&gt;
&lt;p&gt;But what if you have a large-ish number of audio files?
It turns out there isn't a comparably inexpensive and easy way
to handle even modestly sized
batches of files using those just aforementioned tools.
There do exist commercially available alternatives, which will gladly
take those batches of audio files off your hands and return
high quality text. But these are still rather
human labor intensive, which means that they remain expensive.&lt;/p&gt;
&lt;p&gt;The typical going rate is about a $1/minute, although with a bit
of coding effort (typically meaning slicing files into uniform size work units,
farming said work units out to a distributed online on-demand work force
such as mechanical turk, then resplicing the files back together)
apparently can bring the price down to somewhat, say 70 cents/minute.
(The slicing and resplicing is needed for quality assurance as well
as to reduce privacy and security issues.)
That 70 cents a minute is still rather unaffordable to the typical consumer who is a tad long-winded,
much less the odd sort who just happens to have a boat load of such files.
Not to mention the concerns related to having unknown on-demand workers
hearing an early preview of your memoirs
(cf. &lt;a href="https://www.wired.com/2016/04/long-form-voice-transcription/"&gt;Long Form Voice Transcription, Wired, April 2016&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Fortunately, an affordable alternative is emerging, that is,
if you're willing to write a bit of code.
All of the hyperscale cloud api providers have made their deep
learning speech-to-text models available for a small fraction of the
dollar cost of the aforementioned human-corrected transcription services,
and two of them are now appealing enough for use by individuals
or nonprofits to apply to long form content.
Currently you can get cloud api based speech transcription
for
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Cloud-API-Pricing"&gt;prices ranging from 2 to 2.4 cents per minute&lt;/a&gt;,
with an additional allowance free on a monthly basis.
Moreover, they effortlessly handle large batches in a single bound.
You upload your files to the cloud, offloading the
processing from your laptop. Whereas your desktop software or even
embedded operating system software would slow your entire machine down
to a crawl during this process
you could be processing batches of files in parallel
up in the cloud.&lt;/p&gt;
&lt;p&gt;Although there is still that small issue of having to write some code
in order to get this amazing cost and time reduction.
&lt;a href="https://github.com/pluteski/speech-to-text"&gt;I went and wrote said code.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So this amazing capability is generally available
at a reasonable price
to anyone with a laptop, internet connection,
and credit card with an embarrassingly low limit.
But it isn't all peaches and chocolate just yet.
There is a still a ways to go before these APIs are reliably useful
for converting conversational audio recorded in the wild using
mobile phones amidst the usual ambient sounds.
Nonetheless, the results are tantalizingly good enough
and the pricing dramatically low enough that the time has come to dip
my metaphorical toes in the water.&lt;/p&gt;
&lt;p&gt;I intend to write more about my experiences using these
powerful cloud APIs, so stay tuned if this is of interest
to you.  Although I myself tend to cringe when the blogger ends
their post by promising an enticing followup, which invariably never
comes.  But I really do sincerely intend to write more on this.
Of course, that's what they would all say, I'm sure.&lt;/p&gt;
&lt;p&gt;I have already compiled a backlog of interesting results just waiting
to be converted into
intriguing visuals and wrapped in witty yet insightful prose. Promise.&lt;/p&gt;
&lt;p&gt;Actually you don't need to wait until then, as
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Background"&gt;here is some additional background on the project&lt;/a&gt;.
Here are the
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Findings"&gt;results of a preliminary comparison between IBM and Google&lt;/a&gt;.
Here are
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Comparison-over-reference-documents"&gt;results made over reference documents&lt;/a&gt;.
And here are some
&lt;a href="https://github.com/pluteski/speech-to-text/wiki/Comparison-of-transcript-word-count"&gt;results made over audio collected in the wild&lt;/a&gt;.
There are several conclusions that can be drawn from these findings,
which will be the subject of a future post.&lt;/p&gt;
&lt;p&gt;In closing, my findings indicate to me that I should be able to
extract useful text from my recordings,
albeit probably not for the purpose of transcribing meeting minutes.
The transcript text generated by the cloud API is largely recognizable to me, although
that is often in the same way as my handwriting is recognizable to me
because I vaguely remember generating it.
The error rates are still too high
for these transcripts to allow me to reliably parse meaningful
whole sentences much less serve as document of record.&lt;/p&gt;
&lt;p&gt;These cloud APIs do not have the same quality as
do personalized devices. Voicemail transcripts provided by my
phone are subjectively better than what I'm seeing with these cloud APIs.
One reason is that because the cloud APIs cannot be
trained on my voice, although IBM does allow specifying
a &lt;a href="https://www.ibm.com/watson/developercloud/doc/speech-to-text/custom.html#addWords"&gt;custom language model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The accuracy being provided
by the cloud APIs may be suitable for
indexing audio files to make them more easily searchable
(although phonemes might turn out to be the better data unit
for indexing audio than words,
&lt;a href="https://twimlai.com/from-particle-physics-to-audio-ai-with-scott-stephenson/"&gt;e.g., Deepgram&lt;/a&gt;).
I fully expect the current results to
be adequately informative to support meaningful textmining and trend
analysis.&lt;/p&gt;
&lt;p&gt;Upon reviewing the current batch of results it is fascinating
to me how a badly transcribed
sequence of just a few words in a row can turn large swaths of text
into complete gibberish. If you tell a linguist you intend to
&lt;em&gt;recognize speech&lt;/em&gt; they might bid you luck, but an evesdropping
surfer taking that to mean instead
that you intend to &lt;em&gt;wreck a nice beach&lt;/em&gt; might wish you harm.&lt;/p&gt;
&lt;p&gt;We humans are pretty amazing in our ability
to understand each other's muffled speech despite the
poor connection and background noise
much less the tinnitus and what have you.&lt;/p&gt;
&lt;p&gt;Nonetheless, I foresee a not too distant future where
it will be possible to cheaply attain human level accuracy
transcription of speaker-independent long-form speech
from noisy recordings obtained using mobile devices. Culminating in,
as &lt;a href="https://www.wired.com/2016/04/long-form-voice-transcription/"&gt;Wired's Jesse Jarnow&lt;/a&gt; puts it,
'bizarre kinds of unforeseen societal change'.&lt;/p&gt;
&lt;p&gt;Until then, court stenographers shouldn't have much to worry about.&lt;/p&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt;

&lt;script&gt;

var disqus_config = function () {
this.page.url = "https://pluteski.github.io/speech-to-text/"
this.page.identifier = "pluteski-github-io-speech-to-text"
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pluteski-github-io-speech-to-text.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
&lt;/script&gt;

&lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript"&gt;comments.&lt;/a&gt;&lt;/noscript&gt;

&lt;h6&gt;fin&lt;/h6&gt;</content></entry></feed>